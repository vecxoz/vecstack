{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification (with probabilities) + Detailed workflow\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-06 15:55:39.761844: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-09-06 15:55:39.792129: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-09-06 15:55:40.527567: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1757163340.650380   23114 gpu_device.cc:2342] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', message='The y_prob values do not sum to one')\n",
    "warnings.filterwarnings('ignore', message='Skipping variable loading for optimizer')\n",
    "warnings.filterwarnings('ignore', message='X does not have valid feature names')\n",
    "import re\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "np.random.seed(0) # ensure reproducibility\n",
    "np.set_printoptions(suppress=True)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import log_loss\n",
    "# Models\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "# NN\n",
    "import tensorflow as tf\n",
    "# Data is small so do NOT use GPU for simplicity\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "# Suppress Python level warnings from tensorflow\n",
    "tf.get_logger().setLevel('ERROR') \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Input\n",
    "from scikeras.wrappers import KerasClassifier, KerasRegressor\n",
    "# Stacking\n",
    "from vecstack import stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (400, 5)\n",
      "Test shape:  (100, 5)\n"
     ]
    }
   ],
   "source": [
    "n_classes = 3\n",
    "\n",
    "# Create data: 500 example, 5 feature, 3 classes\n",
    "X, y = make_classification(n_samples=500, n_features=5, \n",
    "                           n_informative=3, n_redundant=1, \n",
    "                           n_classes=n_classes, flip_y=0, \n",
    "                           random_state=0)\n",
    "\n",
    "# Make train/test split\n",
    "# As usual in machine learning task we have X_train, y_train, and X_test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "print('Train shape:', X_train.shape)\n",
    "print('Test shape: ', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize 1st level models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_keras_model_1():\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(X_train.shape[1],)))\n",
    "    model.add(Dense(64,\n",
    "                    kernel_initializer='normal', \n",
    "                    activation='relu'))\n",
    "    model.add(Dense(n_classes, \n",
    "                    kernel_initializer='normal', \n",
    "                    activation='softmax'))\n",
    "    model.compile(optimizer='rmsprop', \n",
    "                  loss='sparse_categorical_crossentropy', \n",
    "                  metrics=['categorical_accuracy'])\n",
    "    return model\n",
    "\n",
    "# Caution! All models and parameter values are just \n",
    "# demonstrational and shouldn't be considered as recommended.\n",
    "models_1 = [ \n",
    "    GaussianNB(),\n",
    "    \n",
    "    LogisticRegression(random_state=0),\n",
    "    \n",
    "    ExtraTreesClassifier(random_state=0, n_jobs=-1, \n",
    "                         n_estimators=100, max_depth=3),\n",
    "                         \n",
    "    RandomForestClassifier(random_state=0, n_jobs=-1, \n",
    "                           n_estimators=100, max_depth=3),\n",
    "        \n",
    "    XGBClassifier(random_state=0, n_jobs=-1, learning_rate=0.1, \n",
    "                  n_estimators=100, max_depth=3),\n",
    "                  \n",
    "    LGBMClassifier(random_state=0, n_jobs=-1, learning_rate=0.1, \n",
    "                   n_estimators=100, max_depth=3, verbose=-1),\n",
    "\n",
    "    KerasClassifier(model=build_keras_model_1(), epochs=2, \n",
    "                    batch_size=32, verbose=0)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task:         [classification]\n",
      "n_classes:    [3]\n",
      "metric:       [log_loss]\n",
      "mode:         [oof_pred]\n",
      "n_models:     [7]\n",
      "\n",
      "model  0:     [GaussianNB]\n",
      "    fold  0:  [0.57030626]\n",
      "    fold  1:  [0.28256165]\n",
      "    fold  2:  [0.35609357]\n",
      "    fold  3:  [0.57833219]\n",
      "    fold  4:  [0.60933411]\n",
      "    ----\n",
      "    MEAN:     [0.47932556] + [0.13332980]\n",
      "    FULL:     [0.47932556]\n",
      "\n",
      "    Fitting on full train set...\n",
      "\n",
      "model  1:     [LogisticRegression]\n",
      "    fold  0:  [0.58074650]\n",
      "    fold  1:  [0.27626266]\n",
      "    fold  2:  [0.40797434]\n",
      "    fold  3:  [0.48192230]\n",
      "    fold  4:  [0.73155308]\n",
      "    ----\n",
      "    MEAN:     [0.49569178] + [0.15420632]\n",
      "    FULL:     [0.49569178]\n",
      "\n",
      "    Fitting on full train set...\n",
      "\n",
      "model  2:     [ExtraTreesClassifier]\n",
      "    fold  0:  [0.80523428]\n",
      "    fold  1:  [0.72214703]\n",
      "    fold  2:  [0.72851161]\n",
      "    fold  3:  [0.78859646]\n",
      "    fold  4:  [0.82512546]\n",
      "    ----\n",
      "    MEAN:     [0.77392297] + [0.04137715]\n",
      "    FULL:     [0.77392297]\n",
      "\n",
      "    Fitting on full train set...\n",
      "\n",
      "model  3:     [RandomForestClassifier]\n",
      "    fold  0:  [0.49744464]\n",
      "    fold  1:  [0.30045852]\n",
      "    fold  2:  [0.38866325]\n",
      "    fold  3:  [0.49116102]\n",
      "    fold  4:  [0.51274923]\n",
      "    ----\n",
      "    MEAN:     [0.43809533] + [0.08160752]\n",
      "    FULL:     [0.43809533]\n",
      "\n",
      "    Fitting on full train set...\n",
      "\n",
      "model  4:     [XGBClassifier]\n",
      "    fold  0:  [0.44344452]\n",
      "    fold  1:  [0.19888857]\n",
      "    fold  2:  [0.37032590]\n",
      "    fold  3:  [0.50328894]\n",
      "    fold  4:  [0.46400776]\n",
      "    ----\n",
      "    MEAN:     [0.39599114] + [0.10760951]\n",
      "    FULL:     [0.39599114]\n",
      "\n",
      "    Fitting on full train set...\n",
      "\n",
      "model  5:     [LGBMClassifier]\n",
      "    fold  0:  [0.46739007]\n",
      "    fold  1:  [0.20192817]\n",
      "    fold  2:  [0.34714699]\n",
      "    fold  3:  [0.49378077]\n",
      "    fold  4:  [0.50732813]\n",
      "    ----\n",
      "    MEAN:     [0.40351483] + [0.11560251]\n",
      "    FULL:     [0.40351483]\n",
      "\n",
      "    Fitting on full train set...\n",
      "\n",
      "model  6:     [KerasClassifier]\n",
      "    fold  0:  [0.99057170]\n",
      "    fold  1:  [0.82460226]\n",
      "    fold  2:  [0.70419501]\n",
      "    fold  3:  [0.66333640]\n",
      "    fold  4:  [0.66650450]\n",
      "    ----\n",
      "    MEAN:     [0.76984197] + [0.12494150]\n",
      "    FULL:     [0.76984197]\n",
      "\n",
      "    Fitting on full train set...\n",
      "\n",
      "Result was saved to [./[2025.09.06].[15.55.45].032643.7f0aac.npy]\n"
     ]
    }
   ],
   "source": [
    "S_train_1, S_test_1 = stacking(models_1,                   # list of models\n",
    "                               X_train, y_train, X_test,   # data\n",
    "                               regression=False,           # classification task (if you need \n",
    "                                                           #     regression - set to True)\n",
    "                               mode='oof_pred',            # mode: oof for train set, fit on full \n",
    "                                                           #     train and predict test set once\n",
    "                               needs_proba=True,           # predict probabilities (if you need \n",
    "                                                           #     class labels - set to False) \n",
    "                               save_dir='.',               # save result and log in current dir \n",
    "                                                           #     (to disable saving - set to None)\n",
    "                               metric=log_loss,            # metric: callable\n",
    "                               n_folds=5,                  # number of folds\n",
    "                               stratified=True,            # stratified split for folds\n",
    "                               shuffle=True,               # shuffle the data\n",
    "                               random_state=0,             # ensure reproducibility\n",
    "                               verbose=2)                  # print all info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 3 classes and 7 models so in resulting arrays we expect to see 21 columns.\n",
      "S_train_1 shape: (400, 21)\n",
      "S_test_1 shape:  (100, 21)\n"
     ]
    }
   ],
   "source": [
    "print('We have %d classes and %d models so in resulting arrays \\\n",
    "we expect to see %d columns.' % (n_classes, len(models_1), n_classes * len(models_1)))\n",
    "print('S_train_1 shape:', S_train_1.shape)\n",
    "print('S_test_1 shape: ', S_test_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00083161, 0.01828022, 0.98088816, 0.01605728, 0.02724415,\n",
       "        0.95669857, 0.22361998, 0.20286901, 0.57351102, 0.0365039 ,\n",
       "        0.10297998, 0.86051611, 0.00304293, 0.01768309, 0.97927397,\n",
       "        0.00074538, 0.00395689, 0.99529773, 0.32723224, 0.28824005,\n",
       "        0.38452768],\n",
       "       [0.95026182, 0.04973818, 0.        , 0.89084281, 0.10909541,\n",
       "        0.00006178, 0.55396899, 0.27694952, 0.16908149, 0.85772772,\n",
       "        0.13275189, 0.00952038, 0.9821493 , 0.01676223, 0.00108847,\n",
       "        0.99746299, 0.00249197, 0.00004503, 0.39017811, 0.32593027,\n",
       "        0.28389156]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_train_1[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.38824186, 0.37434678, 0.23741136, 0.307547  , 0.52253329,\n",
       "        0.16991971, 0.31662764, 0.29004533, 0.39332704, 0.27655712,\n",
       "        0.55408115, 0.16936173, 0.58477622, 0.38952848, 0.02569526,\n",
       "        0.72381327, 0.24995384, 0.02623288, 0.30865759, 0.34028247,\n",
       "        0.35106   ],\n",
       "       [0.32313599, 0.67239959, 0.00446442, 0.28539557, 0.62320346,\n",
       "        0.09140096, 0.31999925, 0.36345201, 0.31654874, 0.10054021,\n",
       "        0.81354061, 0.08591918, 0.02924692, 0.95591789, 0.01483521,\n",
       "        0.01189152, 0.94377175, 0.04433673, 0.27213532, 0.52776176,\n",
       "        0.20010303]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_test_1[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our arrays and log were saved in current dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrays:\n",
      "[2025.09.06].[15.55.45].032643.7f0aac.npy\n",
      "\n",
      "Logs:\n",
      "[2025.09.06].[15.55.45].032643.7f0aac.log.txt\n"
     ]
    }
   ],
   "source": [
    "names = sorted(glob('*.npy'))\n",
    "npy_1_name = names[0] # for later use\n",
    "\n",
    "print('Arrays:')\n",
    "for name in names:\n",
    "    print(name)\n",
    "\n",
    "names = sorted(glob('*.log.txt'))\n",
    "log_1_name = names[0] # for later use\n",
    "\n",
    "print('\\nLogs:')\n",
    "for name in names:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize some other 1st level model(s)\n",
    "\n",
    "As we continue to work on the problem we create many other models.  \n",
    "Let's say we want to try more powerful neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_keras_model_2():\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(X_train.shape[1],)))\n",
    "    model.add(Dense(256,\n",
    "                    kernel_initializer='normal', \n",
    "                    activation='relu'))\n",
    "    model.add(Dense(64, \n",
    "                    kernel_initializer='normal', \n",
    "                    activation='relu'))\n",
    "    model.add(Dense(n_classes, \n",
    "                    kernel_initializer='normal', \n",
    "                    activation='softmax'))\n",
    "    model.compile(optimizer='rmsprop', \n",
    "                  loss='sparse_categorical_crossentropy', \n",
    "                  metrics=['categorical_accuracy'])\n",
    "    return model\n",
    "\n",
    "# Caution! All models and parameter values are just \n",
    "# demonstrational and shouldn't be considered as recommended.\n",
    "models_2 = [        \n",
    "    KerasClassifier(model=build_keras_model_2(), epochs=5, \n",
    "                    batch_size=32, verbose=0)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform stacking again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task:         [classification]\n",
      "n_classes:    [3]\n",
      "metric:       [log_loss]\n",
      "mode:         [oof_pred]\n",
      "n_models:     [1]\n",
      "\n",
      "model  0:     [KerasClassifier]\n",
      "    fold  0:  [0.51636764]\n",
      "    fold  1:  [0.22029691]\n",
      "    fold  2:  [0.28535020]\n",
      "    fold  3:  [0.39152662]\n",
      "    fold  4:  [0.43074365]\n",
      "    ----\n",
      "    MEAN:     [0.36885700] + [0.10502070]\n",
      "    FULL:     [0.36885700]\n",
      "\n",
      "    Fitting on full train set...\n",
      "\n",
      "Result was saved to [./[2025.09.06].[15.55.47].511377.c4aa2f.npy]\n"
     ]
    }
   ],
   "source": [
    "S_train_2, S_test_2 = stacking(models_2,                   # list of models\n",
    "                               X_train, y_train, X_test,   # data\n",
    "                               regression=False,           # classification task (if you need \n",
    "                                                           #     regression - set to True)\n",
    "                               mode='oof_pred',            # mode: oof for train set, fit on full \n",
    "                                                           #     train and predict test set once\n",
    "                               needs_proba=True,           # predict probabilities (if you need \n",
    "                                                           #     class labels - set to False) \n",
    "                               save_dir='.',               # save result and log in current dir \n",
    "                                                           #     (to disable saving - set to None)\n",
    "                               metric=log_loss,            # metric: callable\n",
    "                               n_folds=5,                  # number of folds\n",
    "                               stratified=True,            # stratified split for folds\n",
    "                               shuffle=True,               # shuffle the data\n",
    "                               random_state=0,             # ensure reproducibility\n",
    "                               verbose=2)                  # print all info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New arrays and log were saved too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrays:\n",
      "[2025.09.06].[15.55.45].032643.7f0aac.npy\n",
      "[2025.09.06].[15.55.47].511377.c4aa2f.npy\n",
      "\n",
      "Logs:\n",
      "[2025.09.06].[15.55.45].032643.7f0aac.log.txt\n",
      "[2025.09.06].[15.55.47].511377.c4aa2f.log.txt\n"
     ]
    }
   ],
   "source": [
    "names = sorted(glob('*.npy'))\n",
    "\n",
    "print('Arrays:')\n",
    "for name in names:\n",
    "    print(name)\n",
    "    \n",
    "names = sorted(glob('*.log.txt'))\n",
    "\n",
    "print('\\nLogs:')\n",
    "for name in names:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time to collect results\n",
    "\n",
    "After several (many) days of building, optimizing, and testing models we have a lot of files with saved OOF.  \n",
    "At this point we can load and use OOF of specific model or all OOF we have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find specific model\n",
    "\n",
    "We can open logs and find the model of interest.  \n",
    "We can do it programmatically or just open logs in editor.  \n",
    "Name of the `.log.txt` file is the same as the name of corresponding `.npy` file (except extension).  \n",
    "To find columns containing OOF of specific model we use model index from log:\n",
    "* if we predicted class labels - corresponding column index is just model index\n",
    "* if we predicted probabilities - corresponding column index is model index multiplied by number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's open this log: [2025.09.06].[15.55.45].032643.7f0aac.log.txt\n",
      "Let's look what models did we build in those session.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Let's open this log: %s\" % log_1_name)\n",
    "with open(log_1_name) as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "print(\"Let's look what models did we build in those session.\\n\")\n",
    "for line in lines:\n",
    "    if re.search(r'^model [0-9]+', line):\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load specific model OOF\n",
    "\n",
    "Let's say we are interested in `LGBMClassifier`.  \n",
    "We found out that it has index 5.  \n",
    "Then we load target `.npy` file and because of probabilities we need 3 columns from 15 (5 \\* 3) to 18 (5 \\* 3 + 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's load this .npy file: [2025.09.06].[15.55.45].032643.7f0aac.npy\n"
     ]
    }
   ],
   "source": [
    "print(\"Let's load this .npy file: %s\" % npy_1_name)\n",
    "S = np.load(npy_1_name, allow_pickle=True)\n",
    "S_train_lgbm = S[0][:, 15:18]\n",
    "S_test_lgbm = S[1][:, 15:18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00074538, 0.00395689, 0.99529773],\n",
       "       [0.99746299, 0.00249197, 0.00004503],\n",
       "       [0.99599212, 0.00369756, 0.00031032],\n",
       "       [0.00109104, 0.99659281, 0.00231615],\n",
       "       [0.98498265, 0.01489686, 0.00012049]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_train_lgbm[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.72381327, 0.24995384, 0.02623288],\n",
       "       [0.01189152, 0.94377175, 0.04433673],\n",
       "       [0.0896902 , 0.90432675, 0.00598305],\n",
       "       [0.00034138, 0.99091816, 0.00874045],\n",
       "       [0.0001139 , 0.99955083, 0.00033527]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_test_lgbm[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute score of specific model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBMCLassifier log loss: 0.40351483\n"
     ]
    }
   ],
   "source": [
    "print('LGBMCLassifier log loss: %.8f' % log_loss(y_train, S_train_lgbm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load ALL OOF\n",
    "\n",
    "***Note:*** If you load OOF from scratch, don't forget to load `y_train` from initial dataset too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 3 classes and 8 models TOTAL so in resulting arrays we expect to see 24 columns.\n"
     ]
    }
   ],
   "source": [
    "print('We have %d classes and %d models TOTAL so in resulting arrays \\\n",
    "we expect to see %d columns.' % (n_classes, len(models_1) + len(models_2), \n",
    "                                 n_classes * (len(models_1) + len(models_2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: [2025.09.06].[15.55.45].032643.7f0aac.npy\n",
      "Loading: [2025.09.06].[15.55.47].511377.c4aa2f.npy\n",
      "\n",
      "S_train_all shape: (400, 24)\n",
      "S_test_all shape:  (100, 24)\n"
     ]
    }
   ],
   "source": [
    "# Create empty arrays\n",
    "S_train_all = np.zeros((X_train.shape[0], 0))\n",
    "S_test_all = np.zeros((X_test.shape[0], 0))\n",
    "\n",
    "# Load results\n",
    "for name in sorted(glob('*.npy')):\n",
    "    print('Loading: %s' % name)\n",
    "    S = np.load(name, allow_pickle=True)\n",
    "    S_train_all = np.c_[S_train_all, S[0]]\n",
    "    S_test_all = np.c_[S_test_all, S[1]]\n",
    "    \n",
    "print('\\nS_train_all shape:', S_train_all.shape)\n",
    "print('S_test_all shape: ', S_test_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply 2nd level model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final prediction score: 0.37246788\n"
     ]
    }
   ],
   "source": [
    "# Initialize 2nd level model\n",
    "model = XGBClassifier(random_state=0, n_jobs=-1, learning_rate=0.1, \n",
    "                      n_estimators=100, max_depth=3)\n",
    "    \n",
    "# Fit 2nd level model\n",
    "model = model.fit(S_train_all, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict_proba(S_test_all)\n",
    "\n",
    "# Final prediction score\n",
    "print('Final prediction score: %.8f' % log_loss(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
